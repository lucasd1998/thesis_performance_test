{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pyodbc    # pip install pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database connection details\n",
    "server   = 'AZURE_SQL_DATABASE_SERVER_NAME'\n",
    "database = 'AZURE_SQL_DATABASE_DATABASE_NAME'\n",
    "username = 'AZURE_SQL_DATABASE_USERNAME'\n",
    "password = 'AZURE_SQL_DATABASE_PASSWORD'\n",
    "driver   = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "# Connect to Azure SQL Database\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER={driver};SERVER={server};PORT=1433;DATABASE={database};UID={username};PWD={password};'\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Delete all data sets in Azure SQL Database by stored procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all datasets in database by using stored procedure\n",
    "cursor.execute(\"EXEC SP_DeleteAllInspectionData\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Insert 10.000 new data sets in Azure SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variables are required to read the corresponding lines from the CSV sequence file.\n",
    "# For this use case, the first 10,000 lines are read.\n",
    "sequence_range_start = 0\n",
    "sequence_range_end   = 10000\n",
    "\n",
    "# Define lists to store the values from the sequence files that fall within the defined range.\n",
    "order_numbers, serial_numbers, article_names, machine_names = [], [], [], []\n",
    "\n",
    "# Read values from the CSV sequence file\n",
    "with open('sequence_of_inserting_data.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "\n",
    "    # Skip lines until the start of the defined range\n",
    "    for _ in range(sequence_range_start):\n",
    "        next(reader)\n",
    "\n",
    "    # Read lines within the defined range\n",
    "    for _ in range(sequence_range_end - sequence_range_start):\n",
    "        row = next(reader)\n",
    "        order_numbers.append(row[0])\n",
    "        serial_numbers.append(row[1])\n",
    "        article_names.append(row[2])\n",
    "        machine_names.append(row[3])\n",
    "\n",
    "# Zip the individual lists together for easier handling\n",
    "sequence_of_inserting_data = zip(order_numbers, serial_numbers, article_names, machine_names)\n",
    "\n",
    "# Insert 10,000 new datasets into the Azure SQL Database\n",
    "for order_number, serial_number, article_name, machine_name in sequence_of_inserting_data:\n",
    "    \n",
    "    # Prepare the dataset \n",
    "    \n",
    "    # Read the reference dataset for the article\n",
    "    reference_dataset_file_path = os.path.join('Reference_Datasets', \n",
    "                                               f\"reference_dataset_{article_name}.json\")\n",
    "    with open(reference_dataset_file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # Replace data in JSON string for Azure Cosmos DB\n",
    "    json_data.update({\n",
    "        'OrderNumber': order_number,     # Use value from the sequence file\n",
    "        'SerialNumber': serial_number,   # Use value from the sequence file\n",
    "        'MachineName': machine_name,     # Use value from the sequence file\n",
    "    })\n",
    "\n",
    "    # Generate the MeasuredValue for each inspection step\n",
    "    for inspection in json_data['InspectionsAndResults']:\n",
    "        lower_border_value = float(inspection['InspectionLowerBorderValue'])\n",
    "        upper_border_value = float(inspection['InspectionUpperBorderValue'])\n",
    "\n",
    "        measured_value = str(round(random.uniform(lower_border_value, upper_border_value), 2))\n",
    "        inspection['InspectionResultMeasuredValue'] = measured_value\n",
    "\n",
    "    # Execute the insert operation \n",
    "\n",
    "    # Convert data to JSON string\n",
    "    json_data_as_string = json.dumps(json_data)\n",
    "\n",
    "    # Insert the dataset into the Azure SQL Database by executing the stored procedure\n",
    "    cursor.execute(\"{CALL SP_InsertInspectionOperation('\" + json_data_as_string + \"')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Retrieve all currently available serialnumbers in Azure SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all serialnumbers from Azure SQL Database and store it in a list\n",
    "cursor.execute('SELECT DISTINCT SerialNumber FROM InspectionOperations')\n",
    "serialnumbers = [row.SerialNumber for row in cursor.fetchall()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run the delete operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The database contains 10.000 data sets\n",
    "number_of_all_datasets = 10000\n",
    "\n",
    "# Declare list to store mean query durations per iteration\n",
    "query_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare 10 iterations a 1.000 data sets\n",
    "for _ in range(10):\n",
    "\n",
    "    # Declare a list to store the mean operation durations of an iteration (1.000 Datasets)\n",
    "    iteration_durations = []\n",
    "    \n",
    "    # 1.000 data sets are deleted in each delete iteration\n",
    "    for _ in range(1000):\n",
    "        \n",
    "        # Select a random serial number for which the corresponding data record is to be deleted\n",
    "        random_serialnumber = random.choice(serialnumbers)\n",
    "\n",
    "        # Run the CRUD-Operation\n",
    "    \n",
    "        # Record the current timestamp before running the operation  \n",
    "        query_start_time = time.time()\n",
    "\n",
    "        # Execute the delete operation and commit\n",
    "        cursor.execute(f'DELETE FROM InspectionOperations WHERE SerialNumber = \"{random_serialnumber}\"')\n",
    "        connection.commit()\n",
    "        \n",
    "        # Record the current timestamp after running the operation\n",
    "        query_end_time = time.time()\n",
    "\n",
    "        # Calculate the duration time for this operation and append this to list\n",
    "        query_duration = query_end_time - query_start_time\n",
    "        iteration_durations.append(query_duration)\n",
    "\n",
    "        # Remove the affected serial number from the list    \n",
    "        serialnumbers.remove(random_serialnumber)\n",
    "\n",
    "    # Calculate the mean duration for the iteration of 1.000 data sets and store in query_durations list        \n",
    "    mean_duration = sum(iteration_durations) / len(iteration_durations)\n",
    "    query_durations.append([mean_duration, 1, number_of_all_datasets])\n",
    "    \n",
    "    number_of_all_datasets -= 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Saving the recorded operation times in the CSV result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the CSV file in the \"Experiment_Results\" directory\n",
    "filepath = os.path.join(\"Experiment_Results\", \"delete_data.csv\")\n",
    "\n",
    "# Check if the file already exists\n",
    "file_exists = os.path.isfile(filepath)\n",
    "\n",
    "# Open the CSV file in append mode; create the file if it does not exist\n",
    "with open(filepath, 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # If the file does not exist, write the header row\n",
    "    if not file_exists:\n",
    "        writer.writerow(['DurationTime', 'NumberOfProcessedDatasets', 'NumberOfDatasetsInDatabase'])\n",
    "    \n",
    "    # Write the rows of data from the query_durations list\n",
    "    writer.writerows(query_durations)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
