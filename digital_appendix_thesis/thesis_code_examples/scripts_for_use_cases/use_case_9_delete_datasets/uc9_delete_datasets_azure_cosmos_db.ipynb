{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from azure.cosmos import CosmosClient        # pip install azure-cosmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Cosmos DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client    = CosmosClient('AZURE_COSMOS_DB_ENDPOINT', 'AZURE_COSMOS_DB_ACCOUNT_KEY')\n",
    "database  = client.get_database_client('AZURE_COSMOS_DB_DATABASE_NAME')\n",
    "container = database.get_container_client('AZURE_COSMOS_DB_CONTAINER_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve all currently available serialnumbers in Azure Cosmos DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to store all retrieved serialnumbers\n",
    "serialnumbers = []\n",
    "\n",
    "# Retrieve all serialnumbers from database\n",
    "datasets = container.query_items(query = 'SELECT DISTINCT c.SerialNumber FROM c', enable_cross_partition_query = True)\n",
    "\n",
    "# Store the retrieved serialnumbers to list\n",
    "for dataset in datasets:\n",
    "    serialnumbers.append(dataset[\"SerialNumber\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The database contains 100.000 data sets\n",
    "number_of_all_datasets = 100000\n",
    "\n",
    "# Declare list to store mean query durations per iteration\n",
    "query_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare 10 iterations a 10.000 data sets\n",
    "for _ in range(10):\n",
    "    \n",
    "    # Declare a list to store the mean operation durations of an iteration\n",
    "    iteration_durations = []\n",
    "    \n",
    "    # 10.000 data sets are deleted in each delete iteration\n",
    "    for _ in range(10000):\n",
    "        \n",
    "        # Select a random serial number for which the corresponding data record is to be deleted\n",
    "        random_serialnumber = random.choice(serialnumbers)\n",
    "        \n",
    "        # Run the CRUD-Operation\n",
    "    \n",
    "        # Record the current timestamp before running the operation  \n",
    "        query_start_time = time.time()\n",
    "        \n",
    "        # First, the ID and partition key of the data set to be updated must be retrieved\n",
    "        sql_query = f\"SELECT c.id, c.ArticleName FROM c WHERE c.SerialNumber = '{random_serialnumber}'\"\n",
    "\n",
    "        datasets = container.query_items(\n",
    "            query = 'SELECT c.id, c.ArticleName FROM c WHERE c.SerialNumber = @serial_number', \n",
    "            enable_cross_partition_query = True\n",
    "        )\n",
    "        \n",
    "        # Delete the retrieved data set\n",
    "        for dataset in datasets:\n",
    "            container.delete_item(\n",
    "                dataset['id'], \n",
    "                partition_key=dataset['ArticleName']\n",
    "            )\n",
    "        \n",
    "\n",
    "        # Record the current timestamp after running the operation\n",
    "        query_end_time = time.time()\n",
    "\n",
    "        # Calculate the duration time for this operation and append this to list\n",
    "        query_duration = query_end_time - query_start_time\n",
    "        iteration_durations.append(query_duration)\n",
    "\n",
    "        # Remove the affected serial number from the list    \n",
    "        serialnumbers.remove(random_serialnumber)\n",
    "    \n",
    "    # Calculate the mean duration for the iteration of 10.000 data sets and store in query_durations list        \n",
    "    mean_duration = sum(iteration_durations) / len(iteration_durations)\n",
    "    query_durations.append([mean_duration, 1, number_of_all_datasets])\n",
    "    \n",
    "    number_of_all_datasets -= 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving the recorded operation times in the CSV result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the CSV file in the \"Experiment_Results\" directory\n",
    "filepath = os.path.join(\"Experiment_Results\", \"delete_data.csv\")\n",
    "\n",
    "# Check if the file already exists\n",
    "file_exists = os.path.isfile(filepath)\n",
    "\n",
    "# Open the CSV file in append mode; create the file if it does not exist\n",
    "with open(filepath, 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # If the file does not exist, write the header row\n",
    "    if not file_exists:\n",
    "        writer.writerow(['DurationTime', 'NumberOfProcessedDatasets', 'NumberOfDatasetsInDatabase'])\n",
    "    \n",
    "    # Write the rows of data from the query_durations list\n",
    "    writer.writerows(query_durations)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
