{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from azure.cosmos import CosmosClient        # pip install azure-cosmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Cosmos DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client    = CosmosClient('AZURE_COSMOS_DB_ENDPOINT', 'AZURE_COSMOS_DB_ACCOUNT_KEY')\n",
    "database  = client.get_database_client('AZURE_COSMOS_DB_DATABASE_NAME')\n",
    "container = database.get_container_client('AZURE_COSMOS_DB_CONTAINER_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve number of affected datasets by the actual CRUD-Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT VALUE COUNT(1)\n",
    "FROM (\n",
    "    SELECT DISTINCT c.SerialNumber\n",
    "    FROM c\n",
    "    WHERE c.MachineName = 'InspectionMachine1'\n",
    ") AS NumberOfDistinctSerialNumbers\n",
    "\"\"\"\n",
    "\n",
    "# Save the number of dataset affected by the actual use case operation\n",
    "number_of_processed_datasets = list(container.query_items(query=sql_query, enable_cross_partition_query=True))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to store the operation durations for each dataset that is inserted\n",
    "query_durations = []\n",
    "\n",
    "# Currently available datasets in database (e.g. 10.000)\n",
    "database_record_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this use case 10 times per iteration\n",
    "for _ in range(10):\n",
    "\n",
    "    # Record the current timestamp before starting the operation  \n",
    "    query_start_time = time.time()\n",
    "\n",
    "    # Define the SQL query to retrieve the ID and partition key of the datasets to be updated\n",
    "    sql_query = \"SELECT c.id, c.ArticleName FROM c WHERE c.MachineName = 'InspectionMachine1'\"\n",
    "\n",
    "    # Retrieve the datasets matching the query\n",
    "    document_records = list(container.query_items(\n",
    "        query=sql_query,\n",
    "        enable_cross_partition_query=True\n",
    "    ))\n",
    "\n",
    "    # Define a function to update a single dataset\n",
    "    def update_document(item):\n",
    "        # Update the 'UpdateDateTime' attribute of the dataset\n",
    "        container.patch_item(\n",
    "            item=item['id'],\n",
    "            partition_key=item['ArticleName'],\n",
    "            patch_operations=[\n",
    "                {\n",
    "                    'op': 'replace',\n",
    "                    'path': '/UpdateDateTime',\n",
    "                    'value': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Use ThreadPoolExecutor to perform parallel updates, which is more efficient than sequential updates\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        executor.map(update_document, document_records)\n",
    "    \n",
    "    # Record the current timestamp after completing the operation\n",
    "    query_end_time = time.time()\n",
    "\n",
    "    # Calculate the duration of the operation\n",
    "    query_duration = query_end_time - query_start_time\n",
    "    query_durations.append(query_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving the recorded operation times in the CSV result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average duration of all operations in this iteration\n",
    "mean_duration = sum(query_durations) / len(query_durations)\n",
    "\n",
    "# Define the dataset to store\n",
    "dataset_to_store = [[\n",
    "    mean_duration,                # Average duration of operations in this iteration\n",
    "    number_of_processed_datasets, # Number of processed datasets\n",
    "    database_record_count         # Current number of datasets in the database\n",
    "]]\n",
    "\n",
    "# Store values in the CSV result file\n",
    "filepath = os.path.join(\"Experiment_Results\", \"select_to_serialnumber.csv\")\n",
    "file_exists = os.path.isfile(filepath)\n",
    "\n",
    "with open(filepath, 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write header if the file does not exist\n",
    "    if not file_exists:\n",
    "        writer.writerow(['DurationTime', 'NumberOfProcessedDatasets', 'NumberOfDatasetsInDatabase'])\n",
    "    \n",
    "    # Append the dataset to the CSV file\n",
    "    writer.writerows(dataset_to_store)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
