{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from azure.cosmos import CosmosClient        # pip install azure-cosmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Cosmos DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client    = CosmosClient('AZURE_COSMOS_DB_ENDPOINT', 'AZURE_COSMOS_DB_ACCOUNT_KEY')\n",
    "database  = client.get_database_client('AZURE_COSMOS_DB_DATABASE_NAME')\n",
    "container = database.get_container_client('AZURE_COSMOS_DB_CONTAINER_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve all currently available serialnumbers in Azure Cosmos DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to store all retrieved serial numbers from the database\n",
    "serialnumbers = []\n",
    "\n",
    "# Retrieve all distinct serial numbers from the database\n",
    "query = 'SELECT DISTINCT c.SerialNumber FROM c'\n",
    "datasets = container.query_items(query=query, enable_cross_partition_query=True)\n",
    "\n",
    "# Store the retrieved serial numbers in the list\n",
    "for dataset in datasets:\n",
    "    serialnumbers.append(dataset[\"SerialNumber\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to store the operation durations for each dataset that is inserted\n",
    "query_durations = []\n",
    "\n",
    "# Number of processed datasets is 1 because only one dataset is affected by the operation in this use case\n",
    "number_of_processed_datasets = 1\n",
    "\n",
    "# Currently available datasets in database (e.g. 10.000)\n",
    "database_record_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this use case 10 times per iteration\n",
    "for _ in range(10):\n",
    "\n",
    "    sql_query = f'SELECT * FROM c WHERE c.SerialNumber = \"{random.choice(serialnumbers)}\"'\n",
    "\n",
    "    # Record the current timestamp before running the operation  \n",
    "    query_start_time = time.time()\n",
    "    \n",
    "    # Run CRUD-Operation\n",
    "    documents = list(\n",
    "        container.query_items(\n",
    "            query = sql_query, \n",
    "            enable_cross_partition_query = True))\n",
    "    \n",
    "    # Record the current timestamp after running the operation\n",
    "    query_end_time = time.time()\n",
    "\n",
    "    # Calculate the duration time for this operation\n",
    "    query_duration = query_end_time - query_start_time\n",
    "    query_durations.append(query_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving the recorded operation times in the CSV result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average duration of all operations in this iteration\n",
    "mean_duration = sum(query_durations) / len(query_durations)\n",
    "\n",
    "# Define the dataset to store\n",
    "dataset_to_store = [[\n",
    "    mean_duration,                # Average duration of operations in this iteration\n",
    "    number_of_processed_datasets, # Number of processed datasets (1 in this case, since 10,000 datasets are inserted sequentially)\n",
    "    database_record_count         # Number of datasets in the database after inserting 10,000 datasets\n",
    "]]\n",
    "\n",
    "# Store values in the CSV result file\n",
    "filepath = os.path.join(\"Experiment_Results\", \"select_to_serialnumber.csv\")\n",
    "file_exists = os.path.isfile(filepath)\n",
    "\n",
    "with open(filepath, 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write header if the file does not exist\n",
    "    if not file_exists:\n",
    "        writer.writerow(['DurationTime', 'NumberOfProcessedDatasets', 'NumberOfDatasetsInDatabase'])\n",
    "    \n",
    "    # Append the dataset to the CSV file\n",
    "    writer.writerows(dataset_to_store)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
