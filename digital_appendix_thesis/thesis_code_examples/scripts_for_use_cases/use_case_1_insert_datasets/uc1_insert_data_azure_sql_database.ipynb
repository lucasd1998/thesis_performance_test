{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pyodbc    # pip install pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database connection details\n",
    "server   = 'AZURE_SQL_DATABASE_SERVER_NAME'\n",
    "database = 'AZURE_SQL_DATABASE_DATABASE_NAME'\n",
    "username = 'AZURE_SQL_DATABASE_USERNAME'\n",
    "password = 'AZURE_SQL_DATABASE_PASSWORD'\n",
    "driver   = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "# Connect to Azure SQL Database\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER={driver};SERVER={server};PORT=1433;DATABASE={database};UID={username};PWD={password};'\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read csv sequence file to insert data in a specific sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variables are used to specify the range of dataset lines to be read from the CSV sequence file.\n",
    "# For this use case, the first 10,000 lines are read.\n",
    "sequence_range_start = 0\n",
    "sequence_range_end   = 10000\n",
    "\n",
    "# Define lists to store values from the sequence file within the specified range.\n",
    "order_numbers, serial_numbers, article_names, machine_names = [], [], [], []\n",
    "\n",
    "# Read the values from the CSV sequence file\n",
    "with open('sequence_of_inserting_data.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "\n",
    "    # Skip lines before the start of the desired range\n",
    "    for _ in range(sequence_range_start):\n",
    "        next(reader)\n",
    "\n",
    "    # Read lines within the specified range\n",
    "    for _ in range(sequence_range_end - sequence_range_start):\n",
    "        row = next(reader)\n",
    "        order_numbers.append(row[0])\n",
    "        serial_numbers.append(row[1])\n",
    "        article_names.append(row[2])\n",
    "        machine_names.append(row[3])\n",
    "\n",
    "# Combine the individual lists into a single iterable for easier processing\n",
    "sequence_of_inserting_data = zip(order_numbers, serial_numbers, article_names, machine_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare the data set to be inserted and insert it into Azure SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to store the operation durations for each dataset that is inserted\n",
    "query_durations = []\n",
    "\n",
    "# Number of processed datasets is 1 because only one dataset is affected by the operation in this use case\n",
    "number_of_processed_datasets = 1\n",
    "\n",
    "# Number of datasets in the database (e.g., 10,000 after the first iteration)\n",
    "database_record_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for order_number, serial_number, article_name, machine_name in sequence_of_inserting_data:\n",
    "    \n",
    "    # Prepare the dataset\n",
    "    \n",
    "    # Read the reference dataset for the article\n",
    "    reference_dataset_file_path = os.path.join(\n",
    "        'Reference_Datasets', \n",
    "        f\"reference_dataset_{article_name}.json\"\n",
    "    )\n",
    "    \n",
    "    with open(reference_dataset_file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # Update JSON data with values from the sequence file\n",
    "    json_data.update({\n",
    "        'OrderNumber': order_number,     # Set the order number from the sequence file\n",
    "        'SerialNumber': serial_number,   # Set the serial number from the sequence file\n",
    "        'MachineName': machine_name,     # Set the machine name from the sequence file\n",
    "    })\n",
    "\n",
    "    # Generate a measured value for each inspection step\n",
    "    for inspection in json_data['InspectionsAndResults']:\n",
    "        lower_border_value = float(inspection['InspectionLowerBorderValue'])\n",
    "        upper_border_value = float(inspection['InspectionUpperBorderValue'])\n",
    "\n",
    "        # Generate a random measured value within the specified range\n",
    "        measured_value = str(round(random.uniform(lower_border_value, upper_border_value), 2))\n",
    "        inspection['InspectionResultMeasuredValue'] = measured_value\n",
    "\n",
    "    # Run the insert operation\n",
    "\n",
    "    # Convert data to a JSON string\n",
    "    json_data_as_string = json.dumps(json_data)\n",
    "\n",
    "    # Record the start time of the operation\n",
    "    query_start_time = time.time()\n",
    "\n",
    "    # Execute the stored procedure to insert the dataset into Azure SQL Database\n",
    "    cursor.execute(\"{CALL SP_InsertInspectionOperation('\" + json_data_as_string + \"')}\")\n",
    "\n",
    "    # Record the end time of the operation\n",
    "    query_end_time = time.time()\n",
    "\n",
    "    # Calculate the duration of the operation\n",
    "    query_duration = query_end_time - query_start_time\n",
    "    query_durations.append(query_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving the recorded operation times in the CSV result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average duration of all operations in this iteration\n",
    "mean_duration = sum(query_durations) / len(query_durations)\n",
    "\n",
    "# Define the dataset to store\n",
    "dataset_to_store = [[\n",
    "    mean_duration,                # Average duration of operations in this iteration\n",
    "    number_of_processed_datasets, # Number of processed datasets (1 in this case, since 10,000 datasets are inserted sequentially)\n",
    "    database_record_count         # Number of datasets in the database after inserting 10,000 datasets\n",
    "]]\n",
    "\n",
    "# Store values in the CSV result file\n",
    "filepath = os.path.join(\"Experiment_Results\", \"select_to_serialnumber.csv\")\n",
    "file_exists = os.path.isfile(filepath)\n",
    "\n",
    "with open(filepath, 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write header if the file does not exist\n",
    "    if not file_exists:\n",
    "        writer.writerow(['DurationTime', 'NumberOfProcessedDatasets', 'NumberOfDatasetsInDatabase'])\n",
    "    \n",
    "    # Append the dataset to the CSV file\n",
    "    writer.writerows(dataset_to_store)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
